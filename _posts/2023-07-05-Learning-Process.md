---
layout: post
title: Test
image: /assets/image/img5.JPG
categories: [category 1, category 2]
---

## Network analysis
A network assumes some connection between entities. What might be an evidence or trends?

The given project is some kind of documentation for example letter, travel of goods, visitor logs, travel records. They are only a part of the network. To find an appropriate trend we need to see the big picture.

For our projects: It can be data from anywhere. We don’t have to write pages. Main goal is that manage to assemble a data. Analytical tricks should reflect on the last project page. Overall, what did we learn?

## Open Refine

Open refine communicates with the other computer with its own IP. It is a kind of excel.

Informations about synagogues in Germany produced by mainly open refine program. To collect information about the synagogues. It is a reverse methodology of last week.
[maitreya](https://www.si.edu/) they provide a free access. “maitreya” then click only openaccess data.

## Geocoding methodology

it finds coordinates for specific locations. We can create our own map with our corpus. It is better if we use the internetarchive’s wayback machine in our project because it is more accurate than the other sources.

[Nominatim](https://nominatim.openstreetmap.org/ui/search.html)

## Collection Builder

CollectionBuilder is an open-source framework for creating digital collection and exhibit websites that are driven by metadata and powered by modern static web technology. Important thing is the .csv document, because the collectioning process is going according to that document. Important thing is the .csv document, because the collectioning process is going according to that document.

[Collection Builder](https://collectionbuilder.github.io/)

## Topic Modelling
The goal of topic modeling is to uncover hidden semantic structure in a collection of documents and represent them as a set of topics, where each topic represents a group of words that often occur together. These topics are generated based on statistical patterns and relationships between words in the documents.

It assumes that each document is a mixture of various topics and each topic is characterized by a distribution of words. By applying it, the algorithm assigns probabilities to the words in each topic and determines the distribution of topics across documents.

Topic modeling can be applied in various fields such as text mining, information retrieval, recommender systems and social media analysis. It helps organize, summarize and understand large collections of unstructured text data, enabling researchers and analysts to gain insights and make sense of key themes found in the data.
